{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1>Advanced Information Analytics</h1> </center>\n",
    "<center> <h2>A statistical perspective on learning</h2> </center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 – Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>\n",
    "<center> <h3>http://acuna.io</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course roadmap\n",
    "<left><img src=\"./images/unit-04/unit-04-0_aia1.png\" width=\"90%\" align=\"left\"></left>\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "  <li>Small/medium data</li>\n",
    "  <li>Low model complexity</li>\n",
    "  <li>High interpretability</li>\n",
    "  <li>Low computational power</li>    \n",
    "</ul>\n",
    "      </div>    \n",
    "    <div class=\"col-6  right2\">\n",
    "        <ul>\n",
    "  <li>Big data</li>\n",
    "  <li>High model complexity</li>\n",
    "  <li>Low interpretability</li>\n",
    "  <li>High computational power</li>  \n",
    "        </ul>\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline of this unit\n",
    "- Definition of learning\n",
    "- Terminology\n",
    "- Function learning\n",
    "- Statistical learning\n",
    "- Prediction: reduce vs irreducible error\n",
    "- Inference\n",
    "- Parametric models vs non-parametric models\n",
    "- Supervised vs unsupervised learning\n",
    "- Regression vs classification\n",
    "- Interpretability vs flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning\n",
    "- From Merriam-Webster:\n",
    "<div class=\"blockquote2\">\n",
    "    <ol> \n",
    "        <li>The act or experience of one that learns.</li>\n",
    "        <li>Knowledge or skill acquired by instruction or study.</li>\n",
    "        <li>Modification of a behavioral tendency by experience (such as exposure to conditioning.)</li>\n",
    "    </ol>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning in this course\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia2.png\" width=\"25%\" align=\"center\"></center>  \n",
    "<br>\n",
    "Using experience to improve future performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Experience = Data\n",
    "  - Future = Data not seen before\n",
    "  - Performance = Error function or loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is statistical learning?\n",
    "From ISLR:\n",
    "<div class=\"blockquote2\">\n",
    "    <p>Statistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology\n",
    "- What we measure: **features** (*input variables, predictors, or independent variables*.)  \n",
    "\n",
    "- What we want to predict or associate with what we measure: **output** (*label, response, dependent variable*.)  \n",
    "\n",
    "- In the case of unsupervised learning, we do not have outputs.  \n",
    "\n",
    "- We will focus on supervised learning first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology (2)\n",
    "- What we measure: **features**  \n",
    "\n",
    "- What we want to predict or associate with what we measure: **output**  \n",
    "\n",
    "- Identify the features and outputs in the following examples:\n",
    "  - Predict the price of a stock in 6 months from now, on the basis of company performance measures and economic data.\n",
    "  - Predict whether a patient, hospitalized due to a heart attack, will have a second heart attack. The prediction is to be based on demographic, diet and clinical measurements for that patient.\n",
    "  - Identify the numbers in a handwritten ZIP code, from a digitized image.\n",
    "  - Predict the longitude and latitude of a car based on GPS measurements, accelerometer, and gyroscope.\n",
    "  - Predict the hash tags of a tweet based on its text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formalization of learning\n",
    "- We will say that **we want to learn a function $f$ about a phenomenon**.  \n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia4_3.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formalization of learning (2)\n",
    "- Predict the price of a stock in 6 months from now, on the basis of company performance measures and economic data.  \n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "  <li>$x$: ?</li>\n",
    "  <li>$y$: ?</li>\n",
    "  <li>$X$: ?</li>\n",
    "  <li>$Y$: ?</li>            \n",
    "</ul>\n",
    "      </div>\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "    <center><img src=\"./images/unit-04/unit-04-0_aia5_3.png\" width=\"100%\" align=\"center\"></center>\n",
    "</ul>\n",
    "\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formalization of learning (3)\n",
    "- Predict the price of a stock in 6 months from now, on the basis of company performance measures and economic data.  \n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "  <li>$x$: market capitalization, trading volume, stock price, and economic data.</li>\n",
    "  <li>$y$: stock price in 6 months.</li>\n",
    "  <li>$X$: space of all possible market capitalization, volumes, stock prices, and economic data.</li>\n",
    "  <li>$Y$: space of all possible  stock prices in 6 months.</li>            \n",
    "</ul>\n",
    "      </div>\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "    <center><img src=\"./images/unit-04/unit-04-0_aia5_3.png\" width=\"100%\" align=\"center\"></center>\n",
    "</ul>\n",
    "\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formalization of learning (4)\n",
    "- Predict whether a patient, hospitalized due to a heart attack, will have a second heart attack. The prediction is to be based on demographic, diet and clinical measurements for that patient.  \n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "  <li>$x$: ?</li>\n",
    "  <li>$y$: ?</li>\n",
    "  <li>$X$: ?</li>\n",
    "  <li>$Y$: ?</li>            \n",
    "</ul>\n",
    "      </div>\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "    <center><img src=\"./images/unit-04/unit-04-0_aia5_3.png\" width=\"100%\" align=\"center\"></center>\n",
    "</ul>\n",
    "\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formalization of learning (5)\n",
    "- Predict whether a patient, hospitalized due to a heart attack, will have a second heart attack. The prediction is to be based on demographic, diet and clinical measurements for that patient.  \n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "  <li>$x$: Demographics, diet, clinical measure of a patient hospitalized due to a heart attack.</li>\n",
    "  <li>$y$: Whether the patient will have a second heart attack.</li>\n",
    "  <li>$X$: Space of demographics, diet, and clinical data of patients hospitalized for heart attack.</li>\n",
    "  <li>$Y$: True or False.</li>            \n",
    "</ul>\n",
    "      </div>\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "    <center><img src=\"./images/unit-04/unit-04-0_aia8_2.png\" width=\"100%\" align=\"center\"></center>\n",
    "</ul>\n",
    "\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formalization of learning (6)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia9_1.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical formalization of learning (7)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia9_2.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Diabetes dataset\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-7\">\n",
    "        <ul>\n",
    "            <li class=\"blockquote2\">Ten baseline variables including age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of $n$ = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
    "            </li>\n",
    "            <li>What we might want to \"learn\" from this dataset?</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div class=\"col-5\">\n",
    "        <center><img src=\"./images/unit-04/unit-04-0_aia9.png\" width=\"90%\" align=\"center\"></center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A statistical perspective on learning\n",
    "- We wish to learn something from the data $D$.\n",
    "\n",
    "\n",
    "- Statistical learning means that the data have been generated by some unknown *random process*:\n",
    "$$x,y \\thicksim p()$$  \n",
    "\n",
    "\n",
    "- This reads *\"$x,y$ are sampled according to the probability distribution $p(\\cdot)$.\"*  \n",
    "\n",
    "\n",
    "- **We wish to estimate the *unknown $p$* from such data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quick probability review\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "    <center><img src=\"./images/unit-04/unit-04-0_aia10.png\" width=\"90%\" align=\"center\"></center>\n",
    "      </div>\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "            <li>There is a **sample space** of possible events.</li>\n",
    "            <li>An event has a **probability** of happening (instead of a certainty).</li>\n",
    "            <li>Probability theory develops ways of estimating probabilities of events.</li>\n",
    "        </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quick probability review (2)\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "    <center><img src=\"./images/unit-04/unit-04-0_aia10.png\" width=\"90%\" align=\"center\"></center>\n",
    "      </div>\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "            <li>**Probability of an event** $A$ is denoted by $\\;p(A)$</li>\n",
    "            <li>**Conditional probability** selects one subset of the sample space $p(x \\mid y) = p(x,y)/p(y)$</li>\n",
    "            <li>**Independence of events**: if $x_1$ and $x_2$ are independent events then $p(x_1, x_2) = p(x_1)p(x_2)$</li>\n",
    "        </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How bad are we at probabilities?\n",
    "- You go to a friend's house and you know that she has two children.\n",
    "- Suddenly, a girl runs between the two of you, and your friend says \"she is my daughter.\"\n",
    "- What is the probability that the other child is **also** a girl?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\qquad$**Hint**: Conditioning selects one subset of the sample space  \n",
    "\n",
    "$$p(x \\mid y) = p(x,y)/p(y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How bad are we at probabilities? (2)\n",
    "- You go to a friend's house and you know that she has two children.\n",
    "- Suddenly, a girl runs between the two of you, and your friend says \"she is my daughter.\"\n",
    "- What is the probability that the other child is **also** a girl?  \n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia11.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A statistical perspective on learning\n",
    "- Statistical learning refers to a set of approaches for estimating $\\;f$ assuming some noise added into the system, which cannot be predicted from $x$.  \n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia12_2.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why we estimate $f$ ?\n",
    "- Broadly speaking, we want to estimate $f$ to make **predictions** or **inference**, or both.\n",
    "- **Prediction** for a new data point never seen before: $\\;Y = f(X)$\n",
    "- E.g., for the diabetes case, we might want to predict disease progression based on the BMI of a patient.\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia13_2.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why we estimate $f$ ? (2)\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <ul>\n",
    "            <li>Assuming that there is a function such that $$y = f(x) + \\epsilon$$</li>\n",
    "            <li>We wish to minimize some loss function using our estimation $$\\hat{Y} = \\hat{f}(X)$$</li>\n",
    "            <li>How small can we get our loss function?</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div class=\"col-6\">\n",
    "        <center><img src=\"./images/unit-04/unit-04-0_aia14.png\" width=\"90%\" align=\"center\"></center>\n",
    "    </div>\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why we estimate $f$ ? (3)\n",
    "- There is always **reducible error** and **irreducible error**.  \n",
    "\n",
    "- Squared error:   \n",
    "$$\\begin{align}\n",
    "E[(Y-\\hat{Y})^2] &= E[(f(X)+\\epsilon-\\hat{f}(X))^2] \\\\\n",
    "&= E[(f(X)-\\hat{f}(X))^2+2(f(X)-\\hat{f}(X))\\epsilon + \\epsilon^2] \\\\\n",
    "&= E[(f(X)-\\hat{f}(X))^2]+\\overbrace{E[2(f(X)-\\hat{f}(X))\\epsilon]}^0 + E[\\epsilon^2] \\\\\n",
    "&= E[(f(X)-\\hat{f}(X))^2] + E[\\epsilon^2] \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "- $E[(f(X)-\\hat{f}(X))^2]$ is **reducible** variance.\n",
    "- $E[\\epsilon^2]$  is **irreducible** variance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why we estimate $f$ ? (4)\n",
    "- **Inference**: sometimes we want to understand $f$ (look inside)  \n",
    "\n",
    "  - Which predictors are associated with $Y$? e.g., do we need to include gender in the prediction of blood pressure?  \n",
    "  - Relationship between $Y$ and each $X$. e.g., blood pressure is higher for older people?  \n",
    "  \n",
    "  - Is the relationship between $Y$ and $X$ appropriately captured by the model? Is the linear relationship enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we estimate $f$ ?\n",
    "- We estimate using **training data**. For notation, we will assume that we have $n$ training points. $x_{ij}$ is the value of variable $j$ for data point $i$, and $y_i$ is the independent variable for that data point.  \n",
    "\n",
    "- **Parametric methods**\n",
    "  1. Define the form of $f$ (e.g., linear model.)\n",
    "  2. Use a procedure to fit or train the model.  \n",
    "  \n",
    "- **Nonparametric methods**\n",
    "  - These methods don’t make assumptions about $f$.\n",
    "  - Informally, they try to get as close as possible to the training data but not too close.\n",
    "  - In general, the more data, the better the fit, but the harder to intepret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we estimate $f$ ? (2)\n",
    "- Sometimes, the probability distribution that generated the data is known or assumed to be known, except for a set of parameters describing it.  \n",
    "\n",
    "$$x,y \\thicksim p(\\theta)$$  \n",
    "\n",
    "- We need to infer $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two views on statistical learning\n",
    "- **Bayesian statistics**: use prior knowledge about the phenomenon and then combine evidence with that prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, use prior over unknown parameters ($p(\\theta)$) and the likelihood of the data given a parameter value ($p(x,y \\mid \\theta)$). Use Baye's Theorem to infer the data generating distribution.\n",
    "\n",
    "$$\\hat{\\theta} \\thicksim p(\\theta \\mid x,y)= \\frac{p(x,y \\mid \\theta)p(\\theta)}{p(x,y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two views on statistical learning (2)\n",
    "- **Frequentist statistics**:  \n",
    "  - Defines an *estimator* for a parameter based on data.  \n",
    "  \n",
    "  - It assumes a procedure where samples from the same data will be observed an infinite amount of time.  \n",
    "  \n",
    "  - It uses the resampled estimated to build a distribution over estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In this class, we will take a **frequentist view**:  \n",
    "  \n",
    "  - We will use the Maximum Likelihood Estimation (MLE):  \n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max_\\theta p(y \\mid \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Diabetes dataset\n",
    "- Let’s look at the disease progression distribution of the sample.  \n",
    "\n",
    "<br>  \n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia15.png\" width=\"40%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Diabetes dataset (2)\n",
    "- We will assume that disease progression is distribuited according to a Gaussian distribution:\n",
    "\n",
    "$$\\text{disease progression} \\thicksim p(\\mu,\\sigma)$$  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia15.png\" width=\"40%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian distribution\n",
    "- Gaussian or Normal distribution: most common probability distribution.  \n",
    "\n",
    "$$y \\thicksim N(\\mu, \\sigma)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right]$$\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia17.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diabetes dataset and assumptions\n",
    "- We will assume that disease progression ($d$) is distributed according to a Gaussian distribution and each subject is independent of each other.\n",
    "\n",
    "$$p(d_1,d_2,\\ldots,d_{442} \\mid \\mu,\\sigma)=p(d_1 \\mid \\mu,\\sigma)p(d_2 \\mid \\mu,\\sigma) \\cdots p(d_{442} \\mid \\mu,\\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- MLE:  \n",
    "\n",
    "$$\\hat{\\mu}=\\arg\\max_\\mu \\prod_{i=1}^{442} {\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2\\sigma^2}(d_i-\\mu)^2\\right]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we perform MLE?\n",
    "- Let’s work through the math.\n",
    "\n",
    "$$\\begin{align}\n",
    "p(d_1,d_2,\\ldots,d_{442} \\mid \\mu,\\sigma) &= \\prod_{i=1}^{442} {\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2\\sigma^2}(d_i-\\mu)^2\\right]} \\\\\n",
    "p(d_1,d_2,\\ldots,d_{442} \\mid \\mu,\\sigma) &= \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{442}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{442}(d_i-\\mu)^2\\right] \\\\\n",
    "\\end{align}$$\n",
    "  \n",
    "  \n",
    "- Because $p$ is always positive, then we have the following property:  \n",
    "\n",
    "$$\\arg⁡\\max⁡ p⁡(\\cdot) = \\arg⁡\\max⁡ \\log p⁡(\\cdot)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we perform MLE? (2)\n",
    "- Let’s work through the math.\n",
    "\n",
    "$$\\log p(d_1,d_2,\\ldots,d_{442} \\mid \\mu,\\sigma) = 442\\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{442}(d_i-\\mu)^2$$\n",
    "  \n",
    "  \n",
    "- At the end, we want to maximize a **likelihood** function based on two variables: $\\mu$ and $\\sigma$ \n",
    "\n",
    "$$\\arg⁡\\max_{\\mu,\\sigma} l(\\mu,\\sigma)$$  \n",
    "\n",
    "- Let’s focus on estimating $\\mu$ for now:\n",
    "  - How would you use calculus to find the maximum of $l(\\mu,\\sigma)$ with respect to $\\mu$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we perform MLE? (3)\n",
    "- Let’s rewrite to make it clear:  \n",
    "\n",
    "$$l(\\mu) = f_1(\\sigma) - f_2(\\sigma)\\sum_{i=1}^{442}(d_i-\\mu)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we perform MLE? (4)\n",
    "- One way of finding maximum/minimum is to look at where the slopes are zero.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia18.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we perform MLE? (4)\n",
    "- Let’s rewrite to make it clear:  \n",
    "$$\\begin{align}\n",
    "\\frac{dl(\\mu)}{d\\mu} &= 0 \\\\\n",
    "\\frac{dl(\\mu)}{d\\mu}\\left(f_1(\\sigma) - f_2(\\sigma)\\sum (d_i-\\mu)^2\\right) &=0 \\\\\n",
    "- f_2(\\sigma)\\sum \\frac{d(d_i-\\mu)^2}{d\\mu} &= 0 \\\\\n",
    "\\sum (d_i-\\mu) &= 0 \\\\\n",
    "\\sum d_i - \\sum\\mu &= 0 \\\\\n",
    "\\sum d_i - n\\mu &= 0 \\\\\n",
    "\\frac{\\sum d_i}{n} &= \\mu \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "- Simply the empirical average! (Can you do the same with the standard deviation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Diabetes dataset\n",
    "- The **statistical estimation** of the distribution that generates *disease progression* is the mean of the data!  \n",
    "\n",
    "- Also, **maximizing the likelihood under Gaussian** assumption is equivalent to finding the **minimum squared error** between my estimation and the data.\n",
    "- Usually, algorithms try to **minimize the negative loglikelihood (nLL)**, which is the same as **maximizing the loglikelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary of disease progression estimation\n",
    "- We assume a statistical perspective to **learn** dp:\n",
    "  - We assume that we have uncertainty in our process.  \n",
    "  \n",
    "- We define a **model** to make such prediction:\n",
    "  - We define a Gaussian distribution around dp.  \n",
    "  \n",
    "- We use **experience**:\n",
    "  - We use data to find an appropriate Gaussian distribution.  \n",
    "  \n",
    "- We define a **loss function** as performance:\n",
    "  - We use the mean squared error to evaluate performance (any problems with this?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A more interesting problem\n",
    "- The diabetes dataset is intended for understanding disease progression.\n",
    "- Again, the dataset description:\n",
    "<div class=\"blockquote2\">\n",
    "    <p>Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of $n$ = 442 diabetes patients, as well as **the response of interest, a quantitative measure of disease progression one year after baseline**.\n",
    "    </p>\n",
    "</div>\n",
    "- Let’s suppose we want to predict disease progression.\n",
    "  - How can we use this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More interesting statistical learning\n",
    "- Suppose there is a relationship:  \n",
    "$$Y=f(X)+\\epsilon$$  \n",
    "\n",
    "  where $X$ is a set of independent variables and $Y$ is an output variable, and $\\epsilon$ is some noise (e.g., unobserved phenomena, variability of subjects) with mean zero.\n",
    "- Given that $\\epsilon$ is a random variable, then the relationship is stochastic:  \n",
    "$$p(Y \\mid X)\\thicksim N(f(X),\\sigma_\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric method: linear regression\n",
    "- Linear regression assumes a linear relationship between $X$ and $Y$ plus some noise:  \n",
    "\n",
    "$$Y = X\\beta+\\epsilon$$  \n",
    "\n",
    "- The noise ($\\epsilon$) is assumed to be Gaussian.  \n",
    "\n",
    "- Therefore, we find the parameters ($\\beta$) by minimizing the squared error (or equivalently, maximizing the likelihood.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: predicting progression based on BMI\n",
    "- Predict disease progression based on the BMI of a patient.  \n",
    "\n",
    "- **Prediction**:  \n",
    "\n",
    "  Model: $\\;Y = \\beta_0 + \\text{bmi}\\beta_1 + \\epsilon$\n",
    "<br>\n",
    "<br>  \n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia19.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: predicting progression based on BMI (interpretation)\n",
    "-   Model: $\\;y = \\beta_0 + \\text{bmi}\\beta_1 + \\epsilon$  \n",
    "\n",
    "- This is, for each observation ($y_i,\\text{bmi}_i$), we will define:\n",
    "\n",
    "  - $\\mu_i = \\beta_0 + \\text{bmi}\\beta_1$  \n",
    "  \n",
    "  - $p(y_i \\mid \\text{bmi}_i) = \\prod_{i=1}^{n} {\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2\\sigma^2}(y_i-\\mu_i)^2\\right]}$ (\\*)  \n",
    "\n",
    "\n",
    "- Same as before, we want to find best $\\beta_0$ and $\\beta_1$ so as to maximize **(\\*)** or minimize the squared error:\n",
    "$$l(\\beta_0,\\beta_1) = \\sum (y_i-\\mu_i)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: predicting progression based on BMI (interpretation) (2)\n",
    "- Again, we can take the derivatives and set them to zero:  \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{dl(\\beta_0,\\beta_1)}{d\\beta_0} &= \\frac{d\\sum(y_i-\\mu_i)^2}{d\\beta_0} \\\\\n",
    "&= (y_1-\\mu_1)^2 + \\cdots + (y_n-\\mu_n)^2 \\\\  \n",
    "&= (y_1-(\\beta_0+\\beta_1\\text{bmi}_1))^2 + \\cdots \\\\  \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: predicting progression based on BMI (interpretation) (3)\n",
    "- In general, we can express the loss function as: \n",
    "$$l(\\beta) = (Y-X\\beta)^T(Y-X\\beta)$$  \n",
    "\n",
    "\n",
    "- And we can try to find the minimum of that function by:\n",
    "$$\\frac{dl(\\beta)}{d\\beta} = 0$$  \n",
    "\n",
    "\n",
    "- The solution to this is out of the scope of this class.\n",
    "\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning: Linear regression\n",
    "<br>\n",
    "$$ \\text{dp} = b_0 + b_\\text{bmi} * \\text{bmi} + \\epsilon$$\n",
    "<br> \n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia20.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning: Linear regression (2)\n",
    "- Maximizing probability is equivalent to **minimizing squared errors of model**:  \n",
    "$$ \\text{dp} = b_0 + b_\\text{bmi} * \\text{bmi} + \\epsilon$$\n",
    "<br> \n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia21.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning: Linear regression (3)\n",
    "- Maximizing probability is equivalent to **minimizing squared errors of model**:  \n",
    "$$ \\text{dp} = b_0 + b_\\text{bmi} * \\text{bmi} + \\epsilon$$\n",
    "<br> \n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia22.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning: Linear regression (4)\n",
    "- Maximizing probability is equivalent to **minimizing squared errors of model**:  \n",
    "$$ \\text{dp} = b_0 + b_\\text{bmi} * \\text{bmi} + \\epsilon$$\n",
    "<br> \n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia23.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning: Linear regression (5)\n",
    "- Linear regression finds the parameters that maximize the probability of observing the data points.\n",
    "$$ \\text{dp} = -117 + 10.23 * \\text{bmi} + \\epsilon$$\n",
    "<br> \n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia24.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What can we do with our model?\n",
    "<br>\n",
    "$$ \\text{dp} = -117 + 10.23 * \\text{bmi} + \\epsilon$$\n",
    "<br> \n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia24.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nonparametric method: Nearest neighbors\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <p>For a new data point $\\hat{X}$ return the mean $Y$ of the closest $k$ points in the training data:</p>\n",
    "        <ul>\n",
    "            <li>Very simple to implement.</li>\n",
    "            <li>Needs lots of data to work well.</li>\n",
    "            <li>Hard to interpret.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div class=\"col-6\">\n",
    "        <center><img src=\"./images/unit-04/unit-04-0_aia26.png\" width=\"90%\" align=\"center\"></center>\n",
    "    </div>\n",
    "</div>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression vs classification\n",
    "- When the variable we are trying to predict ($Y$) is quantitative, then we talk about **regression**.  \n",
    "\n",
    "- When the variable is categorical (no easy comparison), then we talk about **classification**.  \n",
    "\n",
    "- Examples of classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification: statistical formulation\n",
    "- In regression, we learn a *continuous probability distribution* such as Gaussian describing our outcome variable\n",
    "- In classification, we learn a *discrete probability distribution* for our outcome variable\n",
    "- This is because the variable $y$ takes on discrete values $C_1, C_2, \\dots, C_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: two-class logistic regression for classification\n",
    "\n",
    "- One of the simplest models for classification\n",
    "- It makes use of the Bernoulli probability distribution\n",
    "$$ p(y \\mid \\theta) = \\theta^y (1-\\theta)^{y-1}$$\n",
    "where $y \\in \\{0, 1 \\}$ and represent two classes $C_1$ and $C_2$\n",
    "- Can you give examples of two-class classification problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: logistic regression\n",
    "- More specifically, we map each set of features $x$ into a value between 0 and 1 using the following transformation:\n",
    "- We first do a linear map of $x$ into $z$:\n",
    "\n",
    "$$z = b_0 + b_1 x_1 + \\dots + b_m x_m = x^T b$$\n",
    "\n",
    "- And then, we perform a *non-linear transformation* of $z$\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "which is called the **sigmoid** transform. The sigmoid is constrained between 0 and 1 (plot)\n",
    "- We make $\\sigma(z)$ equal to the probability of $y = 1$ or $\\theta$ for a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: logistic regression (2)\n",
    "- Therefore, the likelihood of each observation is as follows\n",
    "\n",
    "$$p(y_i \\mid x_i, b) = \\theta_i^{y_i} (1 - \\theta_i)^{1 - y_i}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\theta_i = \\sigma(x_i^T b)$$\n",
    "\n",
    "- Therefore, the likelihood of a set of observations is as follows\n",
    "\n",
    "$$p(y_1,\\dots,y_n, \\mid b, x_1, \\dots, x_n ) = \\prod_{i=1}^n p(y_i, \\mid x_i, b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: logistic regression (3)\n",
    "\n",
    "- Maximizing the likelihood is similar to what we did for the Gaussian model\n",
    "- We will express the *negative log-likelihood* and *minimize it*:\n",
    "$$\\begin{align}\n",
    "\\text{nLL}(b) &= - \\log {\\prod_{i=1}^n p(y_i, \\mid x_i, b)}\\\\\n",
    " & = - \\sum_{i=1}^n \\log \\left( \\theta_i^{y_i} (1 - \\theta_i)^{1 - y_i} \\right)\\\\\n",
    " & = - \\sum_{i=1}^n \\left( y_i \\log \\theta_i + (1 - y_i) \\log (1 - \\theta_i) \\right)\n",
    " \\end{align}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: logistic regression (4)\n",
    "- Minimizing by finding the parameters that make the derivative 0 cannot be done in one step\n",
    "- *This does not have a closs solution for logistic regression:*\n",
    "$$ \\nabla \\text{nLL}(b) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: logistic regression (5)\n",
    "- We have to do **gradient descent** where we perform the following operations several times\n",
    "$$ b^{t+1} \\leftarrow b^{t} - \\lambda \\nabla \\text{nLL}(b)$$\n",
    "\n",
    "$$ \\nabla \\text{nLL}(b) = (\\frac{d \\text{nLL}(b)}{d b_0} \\; \\dots \\; \\frac{d \\text{nLL}(b)}{d b_m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: logistic regression (6)\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{d \\text{nLL}(b)}{d b_j} &= \\frac{d}{d b_j} \\left[ - \\sum_{i=1}^n \\left( y_i \\log \\theta_i + (1 - y_i) \\log (1 - \\theta_i) \\right) \\right]\\\\\n",
    "&= - \\sum_{i=1}^n \\left( y_i \\frac{d}{d b_j} \\log \\theta_i + (1 - y_i) \\frac{d}{d b_j} \\log (1 - \\theta_i) \\right)\\\\\n",
    "&= - \\sum_{i=1}^n \\left( y_i  - \\theta_i \\right) x_j  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- Derivation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: logistic regression (7)\n",
    "\n",
    "$$ \\frac{d \\text{nLL}(b)}{d b_j} = - \\sum_{i=1}^n \\left( y_i  - \\theta_i \\right) x_j $$\n",
    "\n",
    "- $y_i - \\theta_i$: how wrong our prediction is\n",
    "- $x_j$: the value of feature $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression: What can we do with our model?\n",
    "\n",
    "- Interpretation is difficult:\n",
    "$$p(y = 1 | x, b) = \\sigma(b_0 + \\sum_{j=1}^{m} b_j x_j) $$\n",
    "- How can we interpret $b_j$?\n",
    "- For example, suppose we can predict whether a customer buys a product based on its price in dollars:\n",
    "$$p(\\text{customer buys} | \\text{price}) = \\sigma(0.2 - \\frac{1}{2} \\text{price}) $$\n",
    "- How to interpret the weight of price?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression: What can we do with our model? (2)\n",
    "\n",
    "- One typical solution is to look at the probability changes around the 50% threshold ($z=0$)\n",
    "$$d = p(y = 1 | x=0 \\text{ but } x_j=1) - \\frac{1}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression: What can we do with our model? (3)\n",
    "- This expression $d$ however is still difficult, but we can do a first order approximation around $z=0$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d & \\approx \\frac{d}{d x_j} \\sigma(z)\\\\\n",
    "& = \\sigma(z) (1 - \\sigma(z)) b_j\\\\\n",
    "& = \\sigma(0) (1 - \\sigma(0)) b_j \\quad \\left[z=0\\right]\\\\\n",
    "& = \\frac{1}{4} b_j\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- Interpret: $p(\\text{customer buys} | \\text{price}) = \\sigma(0.2 - \\frac{1}{2} \\text{price}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accuracy vs. interpretability tradeoff\n",
    "In general:\n",
    " - Simple models are less accurate but more interpretable.\n",
    " - Complex models are more accurate but less interpretable.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia27.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised vs unsupervised learning\n",
    "- **Supervised**:\n",
    "  - Each $X$ is associated with a $Y$.  \n",
    "  \n",
    "- **Unsupervised**:\n",
    "  - We have $X$ but no association.  \n",
    "\n",
    "\n",
    "- Supervised learning is in general easier because we know how well we are building the association.  \n",
    "\n",
    "- Unsupervised learning is harder because there is no clear evaluation method.  \n",
    "\n",
    "- This course will mostly deal with **supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning\n",
    "- There is no output.\n",
    "- It can be seen as learning a function that maps the input into an intermediate representation.\n",
    "- That intermediate representation makes the data easier to interpret.\n",
    "- There should be a map back from such intermediate representation and the original space.\n",
    "- The map from intermediate to feature space ($g$) should be as close as possible to the original input.\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia28_3.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples of unsupervised learning\n",
    "- **Clustering**: cluster the diabetic patients into groups.\n",
    "- **Topic modeling**: describe the content of a set of documents (e.g., tweets) based on topics (soft clustering.)\n",
    "- **Dimensionality reduction**: describe a large set of features into a smaller set of features while retaining the variability of the data.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-04/unit-04-0_aia28_3.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Take-home message\n",
    "1. Statistical learning acknowledges uncertainty and tries to learn a stochastic model of the data.  \n",
    "\n",
    "2. We need to define a model of the data.  \n",
    "\n",
    "3. We need to estimate the parameters of such model.  \n",
    "\n",
    "4. We can use that model to predict or interpret the results.  \n",
    "\n",
    "5. We can use supervised learning to learn a relationship between variables.  \n",
    "\n",
    "6. If variables are not quantitative, we use classification models."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
