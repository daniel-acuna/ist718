{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1>Unit 5</h1> </center>\n",
    "<center> <h1>Introduction to Apache Spark</h1> </center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 – Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>\n",
    "<center> <h3>http://acuna.io</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives:  \n",
    "\n",
    "- Describe Apache Spark.  \n",
    "\n",
    "- List Spark components and their functionalities.  \n",
    "\n",
    "- Use Resilient Distributed Datasets.  \n",
    "\n",
    "- Describe the Spark MLlib.  \n",
    "\n",
    "- Perform Spark Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical data science cycle\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as1.png\" width=\"75%\" align=\"center\"></center>\n",
    "<br>\n",
    "<center>Fom IBM’s Foundational Methdology for Data Science</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical data science cycle (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as2.png\" width=\"85%\" align=\"center\"></center>\n",
    "<br>  \n",
    "<center>Fom IBM’s Foundational Methdology for Data Science</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example\n",
    "- Log files visit to a website – Apache Logs:  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as3.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example (2)\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-12\">\n",
    "        <img src=\"./images/unit-05/unit-05-0_as4.png\" width=\"80%\" align=\"right\">\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "      <left><img src=\"./images/unit-05/unit-05-0_as5.png\" width=\"100%\" align=\"left\"></left>\n",
    "    </div>    \n",
    "    <div class=\"col-6  right2\">\n",
    "      <ol>\n",
    "        <li>Business question based on this data?</li>\n",
    "        <li>Data understanding steps?</li>\n",
    "        <li>Data preparation?</li>\n",
    "      </ol>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example (3)\n",
    "- Let’s assume we want to predict whether time of the day and month predicts the type of response:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as4.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example (4)\n",
    "- With Apache Spark we will see how to take the **raw input data** and transform it non-destructively to clean it up or analyze it.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as7.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "Traditionally:\n",
    "- Hadoop uses single programming model: MapReduce.\n",
    "- It only works on hard drives.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as8.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark\n",
    "- RAM bandwidth has been increasing exponentially.\n",
    "- Spark can perform in-memory computations. \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as9.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Spark?\n",
    "- **Apache Spark** is a fast, in-memory analytics system.\n",
    "- Spark has several high-level tools, including:\n",
    "  - **ML**: a machine learning library.\n",
    "  - **Spark Streaming**: enables high-throughput, fault-tolerant stream processing of live data streams.\n",
    "  - **Spark SQL**: runs SQL and HiveQL queries.\n",
    "  - **GraphX**: an API for graphs and graph-parallel computation.\n",
    "- Spark can be executed in two ways:\n",
    "  - Independent processes on a cluster.\n",
    "  - As a YARN application.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark\n",
    "<div class=\"blockquote2\">\n",
    "Apache Spark™ is a fast and general engine for large-scale data processing.\n",
    "</div>\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as10_2.png\" width=\"70%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Architectural Components\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as11.png\" width=\"45%\" align=\"center\"></center>\n",
    "- Spark applications define various transformations and actions on data, but the actual steps of the script are not executed until an output is requested. \n",
    "- The Spark application can run on different cluster managers such as local, Yarn, Mesos, and Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark 1.6+ vs 2.0+\n",
    "- Spark 1.6+ relied on transformations on arbitrary datasets known as Resilient Distributed Datasets (RDDs.)  \n",
    "\n",
    "- With more flexiblity comes greater power but less performance.  \n",
    "\n",
    "- Spark 2.0+ defines more structure in the form of `DataFrame`, which is similar to a table in SQL and `data.frame` in R. \n",
    "\n",
    "- The newest versions of Spark define `DataSet` which is statically-typed `DataFrame` (more structure.)  \n",
    "\n",
    "- Future Spark ML will only work with DataFrames and not RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Spark 2.0+\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Spark 1.6 (RDDs)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classic Spark v. <2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The SparkContext\n",
    "\n",
    "- The **SparkContext** object performs the following tasks:  \n",
    "\n",
    "  - It connects to the YARN ResourceManager and asks for resources on the Hadoop cluster,  \n",
    "  \n",
    "  - Starts executors on the worker nodes in the cluster that the ResourceManager allocated for the Spark application,  \n",
    "  \n",
    "  - Sends the application code to the executors,  \n",
    "  \n",
    "  - And finally, it sends tasks for the executors to run.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark RDDs\n",
    "- An RDD (resilient distributed dataset) is a fault-tolerant collection of elements that can be operated on in parallel.  \n",
    "\n",
    "- An RDD is an immutable collection that represents:\n",
    "  - A dataset…\n",
    "  - …broken up into a list of partitions.\n",
    "  - A list of dependencies on other RDDs.\n",
    "  - An optional list of preferred block locations for an HDFS file.\n",
    "  - Read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important RDD Concepts\n",
    "- **Lineage**:\n",
    "  - Information about how an RDD was derived from other datasets or other RDDs.\n",
    "  - RDD is not necessarily materialized all the time.\n",
    "  - Lineage captured on disk as \"lineage graph.\"  \n",
    "  \n",
    "- **Persistence**:\n",
    "  - Indicate which RDDs they want to keep in memory. \n",
    "  - User can call \"persist\" method.  \n",
    "  \n",
    "- **Partitioning**:\n",
    "  - RDD elements can be partitioned across machines based on a key in each record.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating RDDs\n",
    "- Can create an initial RDD by applying a transformation to data on disk.  \n",
    "\n",
    "- Can create an initial RDD from a code object.  \n",
    "\n",
    "- Example ways to create an RDD in Spark:\n",
    "  - Use the `parallelize` operation to convert an existing code object into an RDD.\n",
    "  - Use `textFile` operation to convert a text file on HDFS into an RDD.\n",
    "  - Use `sequenceFile` operation to convert a binary file on HDFS into an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "myarray = list(range(1, 20))\n",
    "myarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dist_array = sc.parallelize(myarray) # parallelize version of myarray\n",
    "dist_array # it doesn't do anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Persist\n",
    "- Spark’s persist method:\n",
    "  - Indicates which RDDs to reuse.\n",
    "  - Indicates if you want to replicate across machines.\n",
    "  - Indicates priority of which in-memory data to spill to disk first.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shakespeare = sc.textFile(\"/datasets/shakespeare.txt\")\n",
    "love = shakespeare.filter(lambda x: 'love' in x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit love.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "love.cache()\n",
    "# no effect first time\n",
    "love.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit love.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Disk and Recover\n",
    "- RDD can spill to disk:\n",
    "  - Degrades gracefully (to mapreduce performance.)\n",
    "  - Partitions not in use/lesser use and/or low priority spilled first.  \n",
    "  \n",
    "- Failure Recovery:\n",
    "  - RDD partition that fails are recovered by Yarn/Spark Driver.\n",
    "  - Executer applies lineage to prior RDD (or original data on disk) for that partition.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Disk and Recover (2)\n",
    "- Dependencies: \n",
    "  - Narrow – child partition depends on only one parent partition.\n",
    "    - e.g. `map`, `filter`, `union`, HDFS blocks transformations.\n",
    "  - Wide – multiple child partitions depend on one parent partition.\n",
    "    - e.g. `join` and `groupBy` transformations.\n",
    "    - Materialize intermediate calculations on parent for fault recovery.\n",
    "- Checkpointing:\n",
    "  - Materialize RDD with long lineage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Operations\n",
    "- There are two types of operations that can be done on RDDs:\n",
    "  - **Transformations**: create a new dataset/RDD from an existing one.\n",
    "  - **Actions**: which return a value to the driver program after running a computation on the RDD.  \n",
    "  \n",
    "- Transformations:\n",
    "  - Lazy - they do not compute their results right away.\n",
    "  - \"Coarse-grain\"/Bulk only – no cell level updates.  \n",
    "  \n",
    "- Actions:\n",
    "  - Make RDD materialize and run on cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformations\n",
    "- `map(func)`: returns a new distributed dataset formed by passing each element of the source through the function *func*.\n",
    "- `flatMap(func)`: same as `map` but when multiple key-value pairs are returned.\n",
    "- `filter(func)`: return a new dataset formed by selecting those elements of the source on which *func* returns true. \n",
    "- `reduceByKey(func)`: when called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function *func*.\n",
    "- `sortByKey([ascending],[numTasks])`: when called on a dataset of (K, V) pairs where K implements `Ordered`, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the Boolean **ascending** argument.\n",
    "- `join(otherDataset, [numTasks])`: when called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "- `distinct([numTasks])`: returns a new dataset that contains the distinct elements of the source dataset.\n",
    "- `pipe(command, [envVars])`: pipes each partition of the RDD through the provided shell *command*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Transformations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "neg_values = dist_array.map(lambda x: x-1)\n",
    "print(\"neg_values\", neg_values.collect())\n",
    "large_values = dist_array.filter(lambda y: y > 10)\n",
    "print(\"large_values\", large_values.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actions\n",
    "- `reduce(func`): Aggregate the elements of the dataset using a function *func* (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n",
    "- `foreach(func)`: Runs the function *func* on each element in the dataset.\n",
    "- `count()`: returns the number of elements in the dataset.\n",
    "- `first()`: returns the first element in the dataset.\n",
    "- `take(n)`: returns an array with the first ***n*** elements of the dataset.\n",
    "- `saveAsTextFile(path)`: writes the elements in the dataset out to a file in HDFS (or some other file system.)\n",
    "- `saveAsSequenceFile(path)`: writes the elements to HDFS in the `SequenceFile` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "large_values = dist_array.filter(lambda y: y > 10)\n",
    "large_values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "large_values.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf /tmp/large_values.txt\n",
    "large_values.saveAsTextFile('/tmp/large_values.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%ls /tmp/large_values.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /tmp/large_values.txt/part-00006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordCount in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wordCounts = shakespeare.flatMap(lambda line: line.lower().split()).\\\n",
    "    map(lambda word: (word, 1)).\\\n",
    "    reduceByKey(lambda a, b: a + b).\\\n",
    "    sortBy(lambda x: -x[1])\n",
    "wordCounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Operations on key-value pair RDDs\n",
    "- Operations on key-value pair datasets are at the foundation of Hadoop, Spark 1.6, and MapReduce.\n",
    "  - `groupByKey()`: group values with the same key, ex. `rdd.groupByKey()`.\n",
    "  - `mapValues(f)`: applies function only to values not keys.\n",
    "  - `flatMapValues(f)`: same as `mapValues` when function returns several values.\n",
    "  - `keys()`: RDD with only the keys.\n",
    "  - `values()`: RDD with only the values.  \n",
    "  \n",
    "- Simple operations on RDDs:\n",
    "  - `union(otherRDD)`: makes the union of all key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Join operations on pairs of key-value pair RDDs: join operations\n",
    "- Joins are a fundamental operations of data which compute the **Cartesian Product** between two sets (e.g., two RDDs):\n",
    "$$A \\times B=\\{(a,b) \\mid a \\in A \\wedge b \\in B\\}$$  \n",
    "\n",
    "- Most of the time, joins are paired with a filter to improve performance.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as17.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Join operations on pairs of key-value pair RDDs: join operations (2)\n",
    "- We can use this idea to join key-value pairs and then filter for pairs that have the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = sc.parallelize([(1, 1), (1, 2), (2, 3)])\n",
    "B = sc.parallelize([(1, \"A\"), (2, \"B\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A.join(B).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Join operations on pairs of key-value pair RDDs: example\n",
    "- Given the following dataset, compute the total number of orders per state\n",
    "  - Locations: locationID, state.\n",
    "  - Transactions: transactionID, locationID, number of orders.\n",
    "- Explore `leftOuterJoin` and `rightOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations = sc.parallelize([('loc1', 'NY'), ('loc2', 'NY'), \n",
    "                            ('loc3', 'PA'), ('loc4', 'FL')])\n",
    "transactions = sc.parallelize([(1, 'loc1', 2), (2, 'loc1', 3), \n",
    "                               (3, 'loc2', 5), (4, 'loc5', 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations = sc.parallelize([('loc1', 'NY'), ('loc2', 'NY'), \n",
    "                            ('loc3', 'PA'), ('loc4', 'FL')])\n",
    "transactions = sc.parallelize([(1, 'loc1', 2), (2, 'loc1', 3), \n",
    "                               (3, 'loc2', 5), (4, 'loc5', 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "join_loc_tr = locations.\\\n",
    "    join(transactions.map(lambda x: x[1:]))\n",
    "join_loc_tr.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "join_loc_tr.values().\\\n",
    "    reduceByKey(lambda v1, v2: v1+v2).\\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations.\\\n",
    "    leftOuterJoin(transactions.map(lambda x: x[1:])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations.\\\n",
    "    rightOuterJoin(transactions.map(lambda x: x[1:])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations.\\\n",
    "    fullOuterJoin(transactions.map(lambda x: x[1:])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark 2.0\n",
    "### <span style=\"color:gray\">DataFrames</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames\n",
    "- The problem with RDDs is that they do not have enough structure.  \n",
    "\n",
    "- They are harder to optimize and therefore slow.  \n",
    "\n",
    "- DataFrames aim at solving this by adding structure.  \n",
    "\n",
    "- A DataFrame is a distributed collection of data organized into named columns.  \n",
    "\n",
    "- Similar to Pandas DataFrames but distributed across the cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames (2)\n",
    "- You can read from multiple sources into dataframes.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as20.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames (3)\n",
    "- One preferred source is Parquet files.\n",
    "- Parquet files are datasets store in columns.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as21.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrame operations\n",
    "Creation of dataframes:\n",
    "- From Row Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "raw_data = [Row(state='NY', month='JAN', orders=3),\n",
    "            Row(state='NJ', month='JAN', orders=4),\n",
    "            Row(state='NY', month='FEB', orders=5)\n",
    "           ]\n",
    "data_df = spark.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- From RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rows_rdd = sc.parallelize(raw_data)\n",
    "rows_rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- From files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv('/datasets/potholes_2016.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrame operations (2)\n",
    "- Each column of a DataFrame has the same type.  \n",
    "\n",
    "- DataFrame types can be hierarchical: a column might be a \"DataFrame\" itself.  \n",
    "\n",
    "- **You no longer operate using Python**.  \n",
    "\n",
    "- Instead, you transform DataFrames by **selecting**, **modifying**, **filtering**, **joining**, **grouping**, or **aggregating** using specialized commands similar to SQL.  \n",
    "\n",
    "- Many of these commands are **symbolic representations** of the operations.  \n",
    "\n",
    "- After the transformations, Spark builds an *execution plan* that is optimized for the column types and the operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring a DataFrame\n",
    "- `printSchema()`: shows the datatypes of the dataframe.  \n",
    "\n",
    "- `show()`: prints the first $n$ rows.  \n",
    "\n",
    "- `take()`: return first $n$ of rows.  \n",
    "\n",
    "- `sample(withReplacement, fraction)`: randomly sample rows (approximate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting and modifying a DataFrame\n",
    "- `select(*expressions)`: returns a new DataFrame with columns.  \n",
    "\n",
    "- `withColumn(colName, expression)`: creates a new column based on the expression.  \n",
    "\n",
    "- Expressions are **symbolic operations**.  \n",
    "\n",
    "- Symbolic operations can hold *literal* values and *placeholders* for columns.  \n",
    "\n",
    "- You can perform symbolic operations on both literals and placeholders.  \n",
    "\n",
    "- Some symbolic operations are available in the module `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "locations_df = spark.createDataFrame([\n",
    "    Row(location_id = 'loc1', n_employees=3, state='NY'),\n",
    "    Row(location_id = 'loc2', n_employees=8, state='NY'),\n",
    "    Row(location_id = 'loc3', n_employees=3, state='PA'),\n",
    "    Row(location_id = 'loc4', n_employees=1, state='FL')    \n",
    "])\n",
    "transactions_df = spark.createDataFrame([\n",
    "    Row(transaction_id = 1, location_id = 'loc1', n_orders=2.),\n",
    "    Row(transaction_id = 2, location_id = 'loc1', n_orders=3.),\n",
    "    Row(transaction_id = 3, location_id = 'loc3', n_orders=5.),\n",
    "    Row(transaction_id = 4, location_id = 'loc5', n_orders=5.)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symbolic operations\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "      <ul>\n",
    "        <li>expression =  1 + n_employees</li>\n",
    "        <br>            \n",
    "        <li>To express the previous operation, we need a *literal* value (`1`) and a *placeholder* for the column `n_employees`:</li>\n",
    "        <br>  \n",
    "        <center><img src=\"./images/unit-05/unit-05-0_as25.png\" width=\"80%\" align=\"center\"></center>\n",
    "        <br>\n",
    "        <li>A symbolic operation produces a *column* which itself has multiple operations</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "  <div class=\"col-6\">\n",
    "    <center><img src=\"./images/unit-05/unit-05-0_as26.png\" width=\"70%\" align=\"center\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symbolic operations (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as27.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.select(1 + fn.col('n_employees')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "new_column = 1 + fn.col('n_employees')\n",
    "locations_df.select(new_column.alias('new_column')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.select(new_column.alias('new_column').cast('float')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting and modifying\n",
    "- You can use `select` to subselect columns, modify them, or create new columns.   \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as28.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.\\\n",
    "    select('n_employees',\n",
    "          'location_id',\n",
    "          'state',\n",
    "          (fn.col('n_employees') + 1).alias('n_employees_plus_1'),\n",
    "          (fn.col('n_employees') > 5).alias('more_than_5_empl')\n",
    "          ).\\\n",
    "    show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selecting and modifying (2)\n",
    "- Some operations cannot receive a string representing a column so you must explicitly pass a placeholder or a literal.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as29.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.\\\n",
    "    select(fn.sqrt('n_employees'),\n",
    "           'n_employees',\n",
    "           fn.pow(fn.col('n_employees'), fn.lit(2))\n",
    "          ).\\\n",
    "    show()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filtering\n",
    "- `where(expression)`: select only rows where expression is true.  \n",
    "\n",
    "- Expressions can be complex:\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as30.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.where((fn.col('n_employees') > 2) & \\\n",
    "                  (fn.col('state') == 'PA')).\\\n",
    "         show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as31.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "new_df = locations_df.join(transactions_df, on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# you can look at the execution plan\n",
    "new_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining (left outer join)\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as32.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining (right outer join)\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as33.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining (outer join)\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as34.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping\n",
    "- `groupBy(*expressions)`: groups by a list of expressions.  \n",
    "\n",
    "- Typically, a list of columns:\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as35.png\" width=\"70%\" align=\"center\"></center>  \n",
    "\n",
    "\n",
    "- This does not do anything until we aggregate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.join(transactions_df, on='location_id').\\\n",
    "    groupBy('state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aggregate\n",
    "- There are some special functions that only work on grouped data.  \n",
    "\n",
    "- They are applied using the method `agg(*expressions)`.  \n",
    "\n",
    "- For example:\n",
    "  - `fn.sum`, `fn.stddev`: self explanatory.\n",
    "  - `fn.count`: counts when column is not null.\n",
    "  - `fn.countDistinct`: how many distinct values of a column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aggregate (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as36.png\" width=\"60%\" align=\"center\"></center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.join(transactions_df, on='location_id').\\\n",
    "    groupBy('state').\\\n",
    "    agg(fn.sum('n_orders')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sampling from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "locations_df.sample(withReplacement=True, fraction=1.).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get result as Pandas\n",
    "locations_df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random number generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.range(start=0, end=100000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.range(start=0, end=100000).\\\n",
    "    select('id', \n",
    "           fn.randn().alias('gaussian_sample'), \n",
    "           fn.rand().alias('uniform_sample')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML\n",
    "- Spark ML implements several machine learning algorithms at scale:\n",
    "  - Regression\n",
    "  - Classification\n",
    "  - Recommendation systems  \n",
    "  \n",
    "- It works on Spark DataFrames by performing transformations of the data.  \n",
    "\n",
    "- A typical data science analysis requires several transformations.  \n",
    "\n",
    "- These transformation can be implemented through Pipelines.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML (2)\n",
    "- A model is known as **Estimator** in Spark ML and the typical cycle for such objects is as follows:\n",
    "  1. Define zero or more **input columns**.\n",
    "  2. Define zero or more **output columns**.\n",
    "  3. Define **parameters** of the estimator.\n",
    "  4. **Fit** the estimator, which returns a **fitted model**.\n",
    "  5. Use the fitted model to perform **transformations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Transformers and Estimators\n",
    "- Notice that Spark ML for RDD is getting deprecated.\n",
    "- Preferred method in the future is to work only with DataFrames.\n",
    "- Spark Machine Learning works by creating transformers and estimators and takes as input DataFrames.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as37.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Transformers and Estimators (2)\n",
    "- Estimators **need to learn something from the data**.\n",
    "- For example, if we want to count terms in text, we **need to know how many terms are in all documents**.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as38.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# regression\n",
    "from pyspark.ml import regression\n",
    "# feature engineering\n",
    "from pyspark.ml import feature\n",
    "\n",
    "lr_estimator = regression.\\\n",
    "    LinearRegression(featuresCol=\"feature_column\", \n",
    "                     labelCol='label_column')\n",
    "# to fit I would need a proper feature column with a vector\n",
    "# lr_transformer = lr_estimator.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression (2)\n",
    "- Suppose we wanted to predict number of orders from number of employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "orders_employees = locations_df.join(transactions_df, 'location_id')\n",
    "orders_employees.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature columns\n",
    "- We need to create a feature column first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# this defines a transformer\n",
    "features_transformer = \\\n",
    "    feature.VectorAssembler(inputCols=['n_employees'], \n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features_df = features_transformer.transform(orders_employees)\n",
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we can do the estimation\n",
    "lr_estimator = regression.LinearRegression(featuresCol='features',\n",
    "                                          labelCol='n_orders')\n",
    "lr_transformer = lr_estimator.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "lr_transformer.transform(features_df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# coefficients\n",
    "print('intercept', lr_transformer.intercept)\n",
    "print('coeffs', lr_transformer.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity\n",
    "1. Use Spark 1.6 to load and process `/datasets/income_data.txt` folder into an RDD of Rows\n",
    "1. Create a DataFrame from the RDD\n",
    "1. Use `LinearRegression` to estimate the relationship between `age`, `degree`, and `income`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Pipelines\n",
    "- Data science is all about building data analytic pipelines from raw data to models.\n",
    "- Spark ML Pipeline chains multiple Transformers and Estimators.\n",
    "- Pipelines can be saved and shared.\n",
    "- A Pipeline always starts as an Estimator that needs to be fitted.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as39.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Pipelines (2)\n",
    "- A Pipeline **always** needs to be fitted even when stages are all transformers.\n",
    "- A Pipeline transformer carries the entire process.    \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as40.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Algorithms\n",
    "- Some Estimators are algorithms that work on DataFrames.\n",
    "- For example, `LogisticRegression`.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as41.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark ML: Algorithms (2)\n",
    "- Algorithms can be part of Pipelines.\n",
    "- Pipelines can be combined with other Pipeline transformers.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-05/unit-05-0_as42.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets try to standardize features from income prediction\n",
    "1. Use `feature.StandardScaler` to scale the features\n",
    "1. Put the vector assembler, scaler, and linear regression in one `Pipeline`\n",
    "1. Fit the new estimator (pipeline)\n",
    "1. Compare features with previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# va = feature.VectorAssembler(???)\n",
    "# scaler = feature.StandardScaler(??)\n",
    "# regression = regression.LinearRegression(??)\n",
    "# pipe_estimator = Pipeline(stages=[va, scaler, regression])\n",
    "# pipe_model = pipe_estimator.fit(??)\n",
    "# extract last part of pipeline pipe_model.stages[-1]\n",
    "# and compare coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "- **Apache Spark** is a fast, in-memory computing system that runs on Hadoop.  \n",
    "\n",
    "- An application in Spark has several main components, including a driver program, worker nodes, executors and tasks.  \n",
    "\n",
    "- The `SparkContext` object is a key component of the driver program.  \n",
    "\n",
    "- An RDD is a fault-tolerant collection of elements that can be operated on in parallel.  \n",
    "\n",
    "- There are ways to create an RDD in Spark: the `parallelize` function or a reference to a file in HDFS, `textFile` and `sequenceFile`.  \n",
    "\n",
    "- There are two types of operations that can be done on RDDs: **transformations** and **actions**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
